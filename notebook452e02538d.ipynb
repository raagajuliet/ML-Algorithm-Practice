{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CIFAR-10 Dataset CNN Model achieving 85.97% accuracy with regularization and data augmentation","metadata":{}},{"cell_type":"markdown","source":"This notebook is the result of a series of experiments I conducted on the CIFAR-10 dataset to understand hyperparameter tuning of a Convolutional Neural Network.  It explains the model with the final parameters that achieved the highest results. This model secures a 85.97% accuracy on unseen test data.\n\n# CIFAR-10 dataset\nThe CIFAR-10 dataset contains 60,000 color images of dimension 32 X 32 in 3 channels divided into 10 classes. The training data has 50,000 images and the test data has 10,000. You can read more about the dataset here: https://www.cs.toronto.edu/~kriz/cifar.html \nThis is a mulyi-label image classification problem with 10 labels. The data is equally split between the labels.","metadata":{}},{"cell_type":"markdown","source":"### Import the required libraries","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\nimport numpy as np\nimport pandas as pd\n\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import utils\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the dataset from the keras library and split into train and test set\nThis is the easiest way to load the CIFAR-10 dataset. You can also download the files from the link in the introduction, but requires a lot more steps to bring it to a usable state. ","metadata":{}},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\nprint('Training set shape:', X_train.shape)\nprint('Test set shape:', X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize the train and test data\nConverting to float and dividing each instance by 255 so that all the image pixels are between 0 and 1","metadata":{}},{"cell_type":"code","source":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train = X_train / 255.0\nX_test = X_test / 255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One Hot Encode the labels\n\nThe labels are currently vectors stored as a list with 10 values, all are zero except the correct index for that label will be a 1. \nExample: \n- Airplane --> [1,0,0,0,0,0,0,0,0,0] \n- Automobile --> [0,1,0,0,0,0,0,0,0,0]\n- Bird --> [0,0,1,0,0,0,0,0,0,0]\n\nWe want to split them into separate columns by one hot encoding them","metadata":{}},{"cell_type":"code","source":"y_train = tf.keras.utils.to_categorical(y_train, num_classes)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the train set for a validation set\n\nWe will further split the training set to create a validation set to test model results on. We want to make sure that we don't touch the test set till we're happy with our model and are ready to make predictions off the test set. ","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the CNN Model\n\nThe layout of this model is similar to AlexNet designed by Alex Krizhevsky but with different number of filters, kernel_size etc. \n\nWe create a Sequential model and start adding layers one by one. The first Conv2D layers are preceeded by MaxPooling2D and Dropout layer. Then 3 Conv2D layers are stacked followed by again a pooling and dropout layer followed by 2 fully connected Dense layers leading to an output layer. The kernel_size and pool_sie are the same through out the network. \n\nThe filters double in the size with every layer starting from 128 going up to 512 and coming back down to 256 in the fifth layer. Similar values for neurons were used in the fully connected layer. These settings gave me the best accuracy though it can be computationally expensive. With the help of a GPU on the Kaggle platform, I was able to train this model in approximately an hour.\nI went with the standard activation 'relu' and 'same'padding'\n\nTo stop my model from overfitting, I used the l2 kernel_regularizer and also added dropout layers. This reduced overfitting while also icreasing accuracy by a few percent. I again exprimented with a varity of dropout values to land on this one, using lower dropout of 0.3 for the conv layers and a higher 0.5 for the fully connected layers.","metadata":{}},{"cell_type":"code","source":"def cnn_model():\n    \n    model = Sequential()\n    \n    # First Conv layer\n    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4), input_shape=(32,32,3)))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.3))\n    \n    # Second Conv layer\n    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.3))\n    \n    # Third, fourth, fifth convolution layer\n    model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.3))\n\n    # Fully Connected layers\n    model.add(Flatten())\n    \n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(10, activation='softmax'))\n    \n    model.summary()\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation\n\nAugmenting the training data and introducing random variations of images like rotating them by 15 dgrees, changing width and height etc. made the model generalize better and reduce overfitting while also increasing the accuracy by a bit. It does increase the training time due to the added variations, but is definitely worthit training on a GPU. Don't even think of training this model on a CPU, it will take days.","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(featurewise_center=False,\n                            samplewise_center=False,\n                            featurewise_std_normalization=False,\n                            samplewise_std_normalization=False,\n                            zca_whitening=False,\n                            rotation_range=15,\n                            width_shift_range=0.1,\n                            height_shift_range=0.1,\n                            horizontal_flip=True,\n                            vertical_flip=False)\n\ndatagen.fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Intitialize the model","metadata":{}},{"cell_type":"code","source":"model = cnn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile the model\nPretty standard settings for the loss, optimizer and metrics functions. I did play around with the learning rate a little bit but the idea was to keep it fairly low to let it slowly converge.","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n             optimizer=Adam(lr=0.0003, decay=1e-6),\n             metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the model\n\nI first tried a lower batch_size of 32 which increase the amount of time for every epoch and also converged to a higher accuracy very slowly. Batch_size of 64 converged the model much faster and also slowly increases the model accuracy by a bit at the end.\n\nStarted with 100 epochs but increased it to 125 as the model was slowly converging still at 100 epochs. Overfitting was not a concern as I had applied strong regluarization. If you look at the output below, the model is beginning to achieve 80% accuracy on the validation set around 30-35 epochs, and the convergence after that is very slow.","metadata":{}},{"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 64),\n                    steps_per_epoch = len(X_train) // 64, \n                    epochs = 125, \n                    validation_data= (X_valid, y_valid),\n                    verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the train and val accuracy and loss","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating model on the test set","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions","metadata":{}},{"cell_type":"code","source":"pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\ny_pred = np.argmax(pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\nerrors = y_pred - y_true != 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print Classification Report\nThis gives us a breaksdown of scores per label. We can see from the report below that our model has learned classifiying automobiles, ships and truck with a 90% precision and recall, and around 75-90% on all the other categories","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the predictions","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 5, figsize=(12,12))\naxes = axes.ravel()\n\nfor i in np.arange(25):\n    axes[i].imshow(X_test[i])\n    axes[i].set_title('True: %s \\nPredict: %s' % (labels[y_true[i]], labels[y_pred[i]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the wrong predictions","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 5, figsize=(12,12))\naxes = axes.ravel()\n\nmiss_pred = np.where(y_pred != y_true)[0]\nfor i in np.arange(25):\n    axes[i].imshow(X_test[miss_pred[i]])\n    axes[i].set_title('True: %s \\nPredict: %s' % (labels[y_true[miss_pred[i]]], labels[y_pred[miss_pred[i]]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the pictures above that our model is perofrming really well, all the misclassified images can very easily be misclassified by a human as well. They are very similar to the mislcassfied prediction. ","metadata":{}},{"cell_type":"markdown","source":"### Saving the model\nAlways save the model and weights so that we can use this trained model and experiment with different parameters to recreate it.","metadata":{}},{"cell_type":"code","source":"model.save('cifar10_cnn.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}